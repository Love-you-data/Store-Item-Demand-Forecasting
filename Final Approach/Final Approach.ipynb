{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple ML approach through ensemble of XGBoost and LightGBM models\n",
    "This approach achieved a 12.67808 SMAPE on the private leaderboard (93/462)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading into the environment the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lightgbm) # Hard installation\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRaw <- read.csv(\"../Data/train.csv\")\n",
    "testRaw <- read.csv(\"../Data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need to do this if you execute this code in a Kaggle's kernel. The problem is related with a discrepancy between the number of threads on the Kaggle's machine and the selected by the boosting algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(OpenMPController)\n",
    "omp_set_num_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxDF <- rbind.data.frame(trainRaw[, -ncol(trainRaw)], # Auxiliar DF\n",
    "                          testRaw[, -1]) \n",
    "\n",
    "auxDF$date <- as.character(auxDF$date)\n",
    "auxDF$day <- as.numeric(substr(auxDF$date, 9, 10)) # Day of month\n",
    "auxDF$month <- as.numeric(substr(auxDF$date, 6, 7)) # Month\n",
    "auxDF$year <- as.numeric(substr(auxDF$date, 1, 4)) # Year\n",
    "auxDF$dayWeek <- as.numeric(as.factor(weekdays(as.Date(auxDF$date)))) # Day of week\n",
    "auxDF$date <- NULL\n",
    "\n",
    "# Quarter\n",
    "auxDF$quarter <- auxDF$month / 3 \n",
    "auxDF$quarter <- ifelse(auxDF$quarter <= 1, 1, auxDF$quarter)\n",
    "auxDF$quarter <- ifelse(auxDF$quarter > 1 & auxDF$quarter <= 2, 2, auxDF$quarter)\n",
    "auxDF$quarter <- ifelse(auxDF$quarter > 2 & auxDF$quarter <= 3, 3, auxDF$quarter)\n",
    "auxDF$quarter <- ifelse(auxDF$quarter > 3 & auxDF$quarter <= 4, 4, auxDF$quarter)\n",
    "\n",
    "# Select a useful subset of features\n",
    "train <- auxDF[1:nrow(trainRaw), ]\n",
    "train$sales <- trainRaw$sales\n",
    "train <- train[-which(train$day == 29 & train$month == 2), ]\n",
    "train$day <- NULL\n",
    "\n",
    "# Same to the test DF\n",
    "test <- auxDF[(nrow(trainRaw) + 1):nrow(auxDF), ]\n",
    "test$id <- testRaw$id\n",
    "test <- test[, c(8, 1:7)]\n",
    "test$day <- NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the SMAPE functions in order to provide them as loss functions to the boosting algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMAPEcaret <- function(data, lev = NULL, model = NULL) {\n",
    "  \n",
    "  smapeCaret <- Metrics::smape(data$obs, data$pred)\n",
    "  c(SMAPEcaret = -smapeCaret)\n",
    "  \n",
    "}\n",
    "\n",
    "SMAPElightgbm <- function(preds, dtrain){\n",
    "  \n",
    "  actual <- getinfo(dtrain, \"label\")\n",
    "  score <- Metrics::smape(preds, actual)\n",
    "  \n",
    "  return(list(name = \"SMAPE\", value = score, higher_better = FALSE))\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set the optimal hyperparameters for the XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsXGB <- list() # List to save the models for the different items\n",
    "\n",
    "fitControl <- trainControl(method = 'none',\n",
    "                           summaryFunction = SMAPEcaret)\n",
    "\n",
    "parametersXGB <- expand.grid(nrounds = 100, \n",
    "                             max_depth = 6,\n",
    "                             eta = 0.2, \n",
    "                             gamma = 0.6, \n",
    "                             colsample_bytree = 0.5,\n",
    "                             min_child_weight = 2, \n",
    "                             subsample = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the lightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLGB <- list() # List to save the models for the different items\n",
    "\n",
    "lgbGrid  <-  list(objective = \"regression\", \n",
    "                  min_sum_hessian_in_leaf = 1, \n",
    "                  feature_fraction = 0.7, \n",
    "                  bagging_fraction = 0.7, \n",
    "                  bagging_freq = 5, \n",
    "                  max_bin = 50, \n",
    "                  lambda_l1 = 1, \n",
    "                  lambda_l2 = 1.3, \n",
    "                  min_data_in_bin = 100, \n",
    "                  min_gain_to_split = 10, \n",
    "                  min_data_in_leaf = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model for each item through a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(item in 1:50) {\n",
    "  \n",
    "  cat(item, ' | 50', sep = '') # Progress info\n",
    "  \n",
    "  auxTrain <- train[which(train$item == item), ]\n",
    "  auxTrain$item <- NULL\n",
    "  \n",
    "  # XGBoost\n",
    "  modelsXGB[[item]] <- train(x = auxTrain[, -ncol(auxTrain)],\n",
    "                             y = auxTrain[, ncol(auxTrain)],\n",
    "                             method = 'xgbTree',\n",
    "                             trControl = fitControl,\n",
    "                             tuneGrid = parametersXGB,\n",
    "                             metric = 'SMAPEcaret')\n",
    "  \n",
    "  # LightGBM\n",
    "  label <- auxTrain$sales\n",
    "  auxTrainMatrix <- Matrix::Matrix(as.matrix(auxTrain[, -ncol(auxTrain)]), sparse = TRUE)\n",
    "  \n",
    "  auxTrainLGB <- lgb.Dataset(data = auxTrainMatrix, label = label)\n",
    "  \n",
    "  modelsLGB[[item]] <- lgb.train(params = lgbGrid, data = auxTrainLGB, learning_rate = 0.1,\n",
    "                                 num_leaves = 10, num_threads = 3, nrounds = 500,\n",
    "                                 eval_freq = 20, eval = SMAPElightgbm, verbose = -1)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(item in 1:50) {\n",
    "  \n",
    "  cat(item, ' | 50', sep = '')\n",
    "  \n",
    "  indices <- which(test$item == item)\n",
    "  auxTest <- test[indices, ]\n",
    "  auxTest$item <- NULL\n",
    "  \n",
    "  # XGBoost\n",
    "  auxXGB <- predict(modelsXGB[[item]], auxTest[, -1])\n",
    "  \n",
    "  # LightGBM\n",
    "  auxTest <- Matrix::Matrix(as.matrix(auxTest[, -1]), sparse = TRUE)\n",
    "  \n",
    "  auxLGB <- predict(modelsLGB[[item]], auxTest)\n",
    "  \n",
    "  predsXGB <- c(predsXGB, auxXGB)\n",
    "  predsLGB <- c(predsLGB, auxLGB)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply by the \"magic number\" in order to improve a little the predictions, then do the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsXGB <- predsXGB * 1.03\n",
    "predsLGB <- predsLGB * 1.03\n",
    "test$sales <- rowMeans(data.frame(predsXGB, predsLGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission <- read.csv('../Data/sample_submission.csv')\n",
    "submission$sales <- test$sales\n",
    "write.csv(submission, 'lgb_xgb_ensemble.csv', row.names = FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
